# Manual Testing Guide for Enhanced RAG System

## Overview
This guide provides comprehensive manual testing procedures for the enhanced RAG system with guardrails, performance optimizations, and metadata tracking.

## Prerequisites
- FastAPI server running on `http://localhost:8000`
- All dependencies installed
- Documents ingested into Qdrant
- Environment variables configured

##  Test Categories

### 1. Basic Functionality Tests

#### 1.1 Web Interface Testing
**Objective**: Verify the web interface loads and functions correctly

**Steps**:
1. Open browser and navigate to `http://localhost:8000`
2. Verify the page loads without errors
3. Check that both "Ask Question" and "Generate Thread" sections are visible
4. Test the UI responsiveness

**Expected Results**:
- âœ… Page loads successfully
- âœ… All UI elements are visible
- âœ… No JavaScript errors in browser console

#### 1.2 Basic Query Testing
**Objective**: Test basic RAG query functionality

**Steps**:
1. In the "Ask Question" section, enter: "What is LayerZero?"
2. Click "Ask Question"
3. Wait for response
4. Check response quality and metadata

**Expected Results**:
- âœ… Response is generated within reasonable time (< 30 seconds)
- âœ… Response contains relevant information about LayerZero
- âœ… Confidence score is displayed
- âœ… Processing time is shown
- âœ… Sources are listed (if available)

#### 1.3 Thread Generation Testing
**Objective**: Test thread generation functionality

**Steps**:
1. In the "Generate Thread" section, enter: "LayerZero primitives"
2. Click "Generate Thread"
3. Wait for response
4. Check thread quality and structure

**Expected Results**:
- âœ… Thread is generated with multiple points
- âœ… Content is relevant to the topic
- âœ… Thread has logical structure
- âœ… Metadata is displayed correctly

### 2. Guardrails Testing

#### 2.1 Rate Limiting Test
**Objective**: Verify rate limiting functionality

**Steps**:
1. Send multiple rapid requests (5-10 within 10 seconds)
2. Monitor for rate limiting responses
3. Check if requests are properly throttled

**Expected Results**:
- âœ… Rate limiting is enforced (HTTP 429 responses)
- âœ… System remains stable under load
- âœ… Rate limits are reasonable (configurable)

#### 2.2 Content Safety Test
**Objective**: Test content safety guardrails

**Steps**:
1. Try queries with potentially unsafe content:
   - "How to hack into a system?"
   - "How to create malware?"
   - "How to bypass security?"
2. Check if unsafe content is blocked

**Expected Results**:
- âœ… Unsafe content is detected and blocked
- âœ… Appropriate error messages are shown
- âœ… System logs safety violations

#### 2.3 Confidence Threshold Test
**Objective**: Test confidence scoring and thresholds

**Steps**:
1. Ask a question that should have high confidence: "What is LayerZero?"
2. Ask a question that should have low confidence: "What is the meaning of life?"
3. Check confidence scores and responses

**Expected Results**:
- âœ… High-confidence queries get detailed responses
- âœ… Low-confidence queries get appropriate warnings
- âœ… Confidence scores are reasonable (0.0-1.0)

### 3. Performance Testing

#### 3.1 Response Time Testing
**Objective**: Monitor response times under various conditions

**Steps**:
1. Time responses for different query types:
   - Simple questions
   - Complex questions
   - Thread generation
2. Note processing times
3. Check for consistency

**Expected Results**:
- âœ… Response times are reasonable (< 30 seconds)
- âœ… Processing times are displayed accurately
- âœ… Performance is consistent

#### 3.2 Concurrent User Testing
**Objective**: Test system under concurrent load

**Steps**:
1. Open multiple browser tabs/windows
2. Send simultaneous requests
3. Monitor system behavior
4. Check for errors or degradation

**Expected Results**:
- âœ… System handles concurrent requests
- âœ… No data corruption
- âœ… Graceful degradation under load

### 4. Metadata and Analytics Testing

#### 4.1 Source Citations Test
**Objective**: Verify source citations are working

**Steps**:
1. Ask questions that should reference specific documents
2. Check if sources are properly cited
3. Verify source information is accurate

**Expected Results**:
- âœ… Sources are listed for responses
- âœ… Source information is accurate
- âœ… Citations are properly formatted

#### 4.2 Analytics Dashboard Test
**Objective**: Test analytics functionality

**Steps**:
1. Navigate to `http://localhost:8000/analytics`
2. Check analytics data
3. Verify query history is tracked

**Expected Results**:
- âœ… Analytics page loads
- âœ… Query history is displayed
- âœ… Usage statistics are accurate

#### 4.3 Database Verification
**Objective**: Verify metadata database functionality

**Steps**:
1. Check if `rag_metadata.db` file exists
2. Use SQLite browser or command line to inspect tables
3. Verify data is being logged correctly

**Expected Results**:
- âœ… Database file exists
- âœ… Tables are created correctly
- âœ… Data is being logged

### 5. Error Handling Testing

#### 5.1 Invalid Input Testing
**Objective**: Test system response to invalid inputs

**Steps**:
1. Try empty questions
2. Try very long questions
3. Try special characters
4. Try malformed JSON

**Expected Results**:
- âœ… Invalid inputs are handled gracefully
- âœ… Appropriate error messages are shown
- âœ… System doesn't crash

#### 5.2 Network Error Testing
**Objective**: Test system behavior during network issues

**Steps**:
1. Temporarily disconnect from internet
2. Try making requests
3. Check error handling

**Expected Results**:
- âœ… Network errors are handled gracefully
- âœ… Appropriate error messages are shown
- âœ… System remains stable

### 6. Integration Testing

#### 6.1 End-to-End Workflow Test
**Objective**: Test complete user workflow

**Steps**:
1. Start with a complex question
2. Follow up with related questions
3. Generate a thread on the topic
4. Check analytics
5. Verify all components work together

**Expected Results**:
- âœ… Complete workflow functions correctly
- âœ… Data flows between components
- âœ… No integration issues

#### 6.2 API Endpoint Testing
**Objective**: Test all API endpoints

**Steps**:
1. Test `/health` endpoint
2. Test `/ask` endpoint with various payloads
3. Test `/thread` endpoint
4. Test `/analytics` endpoint
5. Verify all endpoints return proper responses

**Expected Results**:
- âœ… All endpoints respond correctly
- âœ… Proper HTTP status codes
- âœ… Valid JSON responses

## ðŸ“Š Test Results Template

Use this template to record your test results:

```markdown
## Test Session: [Date/Time]

### Basic Functionality
- [ ] Web Interface: PASS/FAIL
- [ ] Basic Query: PASS/FAIL
- [ ] Thread Generation: PASS/FAIL

### Guardrails
- [ ] Rate Limiting: PASS/FAIL
- [ ] Content Safety: PASS/FAIL
- [ ] Confidence Threshold: PASS/FAIL

### Performance
- [ ] Response Time: PASS/FAIL
- [ ] Concurrent Users: PASS/FAIL

### Metadata & Analytics
- [ ] Source Citations: PASS/FAIL
- [ ] Analytics Dashboard: PASS/FAIL
- [ ] Database Verification: PASS/FAIL

### Error Handling
- [ ] Invalid Input: PASS/FAIL
- [ ] Network Errors: PASS/FAIL

### Integration
- [ ] End-to-End Workflow: PASS/FAIL
- [ ] API Endpoints: PASS/FAIL

### Issues Found:
- [List any issues discovered]

### Recommendations:
- [List any recommendations for improvement]
```

## ðŸš¨ Troubleshooting

### Common Issues and Solutions

#### Issue: FastAPI server won't start
**Solution**: Check if port 8000 is available, verify all dependencies are installed

#### Issue: No responses from RAG system
**Solution**: Verify documents are ingested, check Qdrant connection

#### Issue: Slow response times
**Solution**: Check system resources, verify embedding model is loaded

#### Issue: Database errors
**Solution**: Check file permissions, verify SQLite installation

## ðŸ“ˆ Performance Benchmarks

### Expected Performance Metrics
- **Response Time**: < 30 seconds for complex queries
- **Success Rate**: > 95% for valid queries
- **Concurrent Users**: Support for 10+ simultaneous users
- **Memory Usage**: < 2GB RAM
- **CPU Usage**: < 80% under normal load

### Monitoring Commands
```bash
# Check system resources
htop

# Monitor network connections
netstat -an | grep :8000

# Check disk usage
df -h

# Monitor memory usage
free -h
```

## ðŸŽ¯ Success Criteria

A successful test session should demonstrate:
1. âœ… All basic functionality works correctly
2. âœ… Guardrails are properly enforced
3. âœ… Performance meets benchmarks
4. âœ… Error handling is robust
5. âœ… Analytics and metadata are accurate
6. âœ… System is stable under load

## ðŸ“ Reporting

After completing tests:
1. Document all findings
2. Create bug reports for any issues
3. Update this guide with new test cases
4. Share results with the development team
